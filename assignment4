import os
import mimetypes
import openai
import requests
from pypdf import PdfReader
from docx import Document
import pandas as pd
from bs4 import BeautifulSoup

def get_text_from_file(file_path):
    ext = os.path.splitext(file_path)[1].lower()
    try:
        if ext == '.txt':
            with open(file_path, 'r', encoding='utf-8') as f:
                return f.read()
        elif ext == '.pdf':
            reader = PdfReader(file_path)
            return "\n".join(page.extract_text() for page in reader.pages if page.extract_text())
        elif ext == '.docx':
            doc = Document(file_path)
            return "\n".join([para.text for para in doc.paragraphs])
        elif ext == '.csv':
            df = pd.read_csv(file_path)
            return df.to_string(index=False)
        else:
            raise ValueError(f"Unsupported file type: {ext}")
    except Exception as e:
        return f"[Error reading {file_path}: {e}]"

def get_text_from_url(url):
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, 'html.parser')
        return soup.get_text(separator="\n", strip=True)
    except Exception as e:
        return f"[Error fetching {url}: {e}]"

def collect_text_from_sources(sources):
    all_text = []
    for src in sources:
        if src.startswith("http://") or src.startswith("https://"):
            print(f"Fetching from URL: {src}")
            content = get_text_from_url(src)
            all_text.append(f"--- {src} ---\n{content}\n")
        elif os.path.isfile(src):
            print(f"Reading local file: {src}")
            content = get_text_from_file(src)
            all_text.append(f"--- {src} ---\n{content}\n")
        else:
            print(f"Warning: Invalid source skipped: {src}")
    return "\n".join(all_text)

def query_openai(prompt, text, model="gpt-3.5-turbo"):
    max_tokens = 3500  # Leave room for response
    words = text.split()
    chunks = []

    while words:
        chunk = words[:4000]
        words = words[4000:]
        chunks.append(" ".join(chunk))

    responses = []
    for i, chunk in enumerate(chunks):
        print(f"Sending chunk {i+1}/{len(chunks)} to OpenAI...")
        response = openai.ChatCompletion.create(
            model=model,
            messages=[
                {"role": "system", "content": "You are a helpful assistant."},
                {"role": "user", "content": f"{prompt}\n\n{chunk}"}
            ],
            temperature=0.7
        )
        responses.append(response.choices[0].message.content.strip())

    return "\n\n---\n\n".join(responses)

def main():
    openai.api_key = os.getenv("OPENAI_API_KEY")
    if not openai.api_key:
        print("Error: OPENAI_API_KEY environment variable not set.")
        return

    print("Enter input sources (URLs or local file paths). Type 'done' to finish:")
    sources = []
    while True:
        src = input("Source: ").strip()
        if src.lower() == 'done':
            break
        if src:
            sources.append(src)

    if not sources:
        print("No sources provided. Exiting.")
        return

    query = input("Enter query (leave empty for summarization): ").strip()
    if not query:
        query = "Summarize the content."

    output = input("Enter output file path (leave empty to print on screen): ").strip()

    combined_text = collect_text_from_sources(sources)
    result = query_openai(query, combined_text)

    if output:
        with open(output, 'w', encoding='utf-8') as f:
            f.write(result)
        print(f"Result saved to {output}")
    else:
        print("\n--- Result ---\n")
        print(result)

if __name__ == '__main__':
    main()
