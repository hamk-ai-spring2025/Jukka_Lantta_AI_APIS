import streamlit as st
import openai
import httpx
import os
import time
from dotenv import load_dotenv

# Load .env file
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

# Check if LM Studio is running
def is_lmstudio_available():
    try:
        httpx.get("http://localhost:1234/v1/models", timeout=3)
        return True
    except httpx.RequestError:
        return False

# Model mapping without relying on replace
model_mapping = {
    "OpenAI-GPT-3.5": "gpt-3.5-turbo",
    "OpenAI-GPT-3.5-16k": "gpt-3.5-turbo-16k",
    "OpenAI-GPT-4": "gpt-4",
    "OpenAI-GPT-4o": "gpt-4o",
    "DeepSeek-Local": "deepseek-r1-distill-qwen-7b"
}

# Token cost per 1K tokens in USD (approximate public rates as of 2024)
token_cost_per_1k = {
    "gpt-3.5-turbo": 0.0015,
    "gpt-3.5-turbo-16k": 0.003,
    "gpt-4": 0.03,
    "gpt-4o": 0.005,
    "deepseek-r1-distill-qwen-7b": 0.0  # Local model assumed free
}

# Streamlit UI
st.set_page_config(page_title="Multi-LLM Chat", layout="centered")
st.title("🧠 Multi-LLM Chat Interface")

model = st.selectbox(
    "Select LLM Model",
    options=[
        f"OpenAI-GPT-3.5 ($0.0015/1K tokens)",
        f"OpenAI-GPT-3.5-16k ($0.0030/1K tokens)",
        f"OpenAI-GPT-4 ($0.0300/1K tokens)",
        f"OpenAI-GPT-4o ($0.0050/1K tokens)",
        f"DeepSeek-Local ($0.0000/1K tokens)"
    ]
)
prompt = st.text_area("Enter your prompt:", height=150)

if st.button("Send") and prompt:
    with st.spinner("Getting response..."):
        try:
            model_clean = model.split(" (" )[0]  # Strip pricing info
            model_id = model_mapping[model_clean]
            start_time = time.time()
            retries = 3

            if model_clean.startswith("OpenAI"):
                chat = openai.ChatCompletion.create(
                    model=model_id,
                    messages=[{"role": "user", "content": prompt}]
                )
                response = chat["choices"][0]["message"]["content"]
                tokens = chat.get("usage", {})

            elif model_clean == "DeepSeek-Local":
                for attempt in range(retries):
                    try:
                        deepseek_response = httpx.post(
                            "http://localhost:1234/v1/chat/completions",
                            json={"model": model_id, "messages": [{"role": "user", "content": prompt}]},
                            timeout=60, follow_redirects=True
                        )
                        json_data = deepseek_response.json()
                        response = json_data["choices"][0]["message"]["content"]
                        tokens = json_data.get("usage", {})
                        break
                    except Exception as err:
                        if attempt < retries - 1:
                            time.sleep(1)
                            continue
                        else:
                            raise err
            else:
                response = "Unknown model"
                tokens = {}

            elapsed = time.time() - start_time
            st.success(f"Response (⏱ {elapsed:.1f} seconds):")
            st.text_area("Model response:", value=response, height=150, disabled=True)

            if tokens:
                prompt_tokens = tokens.get('prompt_tokens', 0)
                completion_tokens = tokens.get('completion_tokens', 0)
                total_tokens = tokens.get('total_tokens', 0)
                st.markdown(f"**🔢 Tokens Used:** Prompt: {prompt_tokens} | Completion: {completion_tokens} | Total: {total_tokens}")
                if model_id in token_cost_per_1k:
                    cost = total_tokens / 1000 * token_cost_per_1k[model_id]
                    st.markdown(f"**💰 Estimated Cost:** ${cost:.4f}")

        except Exception as e:
            st.error(f"Error: {str(e)}")

if st.button("Test LM Studio Connection"):
    if is_lmstudio_available():
        st.success("✅ LM Studio is running and reachable.")
    else:
        st.error("❌ LM Studio is NOT reachable at http://localhost:1234")
